{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u6N-NiPtBOZ"
      },
      "outputs": [],
      "source": [
        "# Instal dependencies\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "aMMMNw1VxaHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the wikitext dataset\n",
        "subset_dataset = \"wikitext-2-v1\"\n",
        "full_dataset = \"wikitext-103-v1\" # 50x bigger\n",
        "dataset = load_dataset(\"wikitext\", subset_dataset)\n",
        "\n",
        "# Display the dataset\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEVvxAlHuDUL",
        "outputId": "8ff259e8-5f1b-46b6-89e7-aad0e63d386c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 4358\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 36718\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3760\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Set the pad token to the EOS token if it's not already defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Function to encode examples using the tokenizer\n",
        "def encode(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "# Encode the dataset\n",
        "dataset = dataset.map(encode, batched=True)\n",
        "dataset.set_format(type='torch', columns=['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DV-2YRHxlMa",
        "outputId": "9baf62d2-c09e-4eaa-8605-f88559212db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnectedModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=512):\n",
        "        super(FullyConnectedModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # x needs to be long here\n",
        "        x = x.mean(dim=1)  # Sum or average embeddings\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Init Model\n",
        "model = FullyConnectedModel(vocab_size=len(tokenizer))\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "H2i102RGxntQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusting train_epoch to fix label issues\n",
        "def train_epoch(model, data_loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        inputs = batch['input_ids'].to(device)  # Keep inputs as Long for embedding\n",
        "        labels = torch.roll(inputs, -1, dims=1)  # Shift inputs for next-token prediction\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Adjust labels for loss calculation (assuming single token prediction for simplification)\n",
        "        loss = loss_fn(outputs, labels[:, -1])  # Take the last token's output vs shifted label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def evaluate_perplexity(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            labels = torch.roll(inputs, -1, dims=1)[:, -1]  # Last token prediction, labels are indices\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs  # Ensure this is [N, C]\n",
        "\n",
        "            loss = loss_fn(outputs, labels)  # Check that outputs are [N, C] and labels are [N]\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "Z9JWJ81RvcI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling Experiment\n",
        "data_sizes = [1000, 5000, 10000, 15000, 20000, 25000, 30000, 35000]  # Define different sizes to test\n",
        "results = {}\n",
        "for size in data_sizes:\n",
        "    # Create a subset of the dataset\n",
        "    subset = Subset(dataset['train'], indices=range(size))\n",
        "    train_loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
        "        print(f'Training Size: {size}, Epoch: {epoch+1}, Loss: {train_loss}')\n",
        "\n",
        "    # Evaluate Perplexity\n",
        "    perplexity = evaluate_perplexity(model, train_loader, loss_fn, device)\n",
        "    print(f'Training Size: {size}, Perplexity: {perplexity}')\n",
        "    results[size] = perplexity\n",
        "\n",
        "    print(\"Results:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNOrR3hzxJYx",
        "outputId": "b1bf9ed7-2c48-4e58-fd51-e874a65c958a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Size: 1000, Epoch: 1, Loss: 6.236616432666779\n",
            "Training Size: 1000, Epoch: 2, Loss: 3.5576252788305283\n",
            "Training Size: 1000, Epoch: 3, Loss: 3.1031273901462555\n",
            "Training Size: 1000, Epoch: 4, Loss: 2.958975672721863\n",
            "Training Size: 1000, Epoch: 5, Loss: 2.9033003747463226\n",
            "Training Size: 1000, Perplexity: 17.348674774169922\n",
            "Results: {1000: 17.348674774169922}\n",
            "Training Size: 5000, Epoch: 1, Loss: 3.692117724237563\n",
            "Training Size: 5000, Epoch: 2, Loss: 2.968621881702278\n",
            "Training Size: 5000, Epoch: 3, Loss: 2.8175183232826524\n",
            "Training Size: 5000, Epoch: 4, Loss: 2.698087683206872\n",
            "Training Size: 5000, Epoch: 5, Loss: 2.61581422256518\n",
            "Training Size: 5000, Perplexity: 12.298166275024414\n",
            "Results: {1000: 17.348674774169922, 5000: 12.298166275024414}\n",
            "Training Size: 10000, Epoch: 1, Loss: 3.183851831278224\n",
            "Training Size: 10000, Epoch: 2, Loss: 2.7666637472286344\n",
            "Training Size: 10000, Epoch: 3, Loss: 2.5707231236111587\n",
            "Training Size: 10000, Epoch: 4, Loss: 2.454671415553731\n",
            "Training Size: 10000, Epoch: 5, Loss: 2.3823550947152885\n",
            "Training Size: 10000, Perplexity: 10.060457229614258\n",
            "Results: {1000: 17.348674774169922, 5000: 12.298166275024414, 10000: 10.060457229614258}\n",
            "Training Size: 15000, Epoch: 1, Loss: 2.6036425397751177\n",
            "Training Size: 15000, Epoch: 2, Loss: 2.3840134592766455\n",
            "Training Size: 15000, Epoch: 3, Loss: 2.281809155991737\n",
            "Training Size: 15000, Epoch: 4, Loss: 2.212723778156524\n",
            "Training Size: 15000, Epoch: 5, Loss: 2.162349149521361\n",
            "Training Size: 15000, Perplexity: 8.110535621643066\n",
            "Results: {1000: 17.348674774169922, 5000: 12.298166275024414, 10000: 10.060457229614258, 15000: 8.110535621643066}\n",
            "Training Size: 20000, Epoch: 1, Loss: 2.418574958944473\n",
            "Training Size: 20000, Epoch: 2, Loss: 2.2752606990619206\n",
            "Training Size: 20000, Epoch: 3, Loss: 2.1915758482564374\n",
            "Training Size: 20000, Epoch: 4, Loss: 2.1350412947682145\n",
            "Training Size: 20000, Epoch: 5, Loss: 2.0866882045048114\n",
            "Training Size: 20000, Perplexity: 7.5504984855651855\n",
            "Results: {1000: 17.348674774169922, 5000: 12.298166275024414, 10000: 10.060457229614258, 15000: 8.110535621643066, 20000: 7.5504984855651855}\n",
            "Training Size: 25000, Epoch: 1, Loss: 2.3223714269030733\n",
            "Training Size: 25000, Epoch: 2, Loss: 2.198374518653011\n",
            "Training Size: 25000, Epoch: 3, Loss: 2.1206489581891033\n",
            "Training Size: 25000, Epoch: 4, Loss: 2.0587712157412867\n",
            "Training Size: 25000, Epoch: 5, Loss: 1.9984214976620491\n",
            "Training Size: 25000, Perplexity: 7.670215129852295\n",
            "Results: {1000: 17.348674774169922, 5000: 12.298166275024414, 10000: 10.060457229614258, 15000: 8.110535621643066, 20000: 7.5504984855651855, 25000: 7.670215129852295}\n",
            "Training Size: 30000, Epoch: 1, Loss: 2.1956455262739265\n",
            "Training Size: 30000, Epoch: 2, Loss: 2.105753358747405\n",
            "Training Size: 30000, Epoch: 3, Loss: 2.0363719188836593\n",
            "Training Size: 30000, Epoch: 4, Loss: 1.9725352533336387\n",
            "Training Size: 30000, Epoch: 5, Loss: 1.9197230322528749\n",
            "Training Size: 30000, Perplexity: 6.471180438995361\n",
            "Results: {1000: 17.348674774169922, 5000: 12.298166275024414, 10000: 10.060457229614258, 15000: 8.110535621643066, 20000: 7.5504984855651855, 25000: 7.670215129852295, 30000: 6.471180438995361}\n",
            "Training Size: 35000, Epoch: 1, Loss: 2.100449265466094\n",
            "Training Size: 35000, Epoch: 2, Loss: 2.008459497195493\n",
            "Training Size: 35000, Epoch: 3, Loss: 1.9514866689223476\n",
            "Training Size: 35000, Epoch: 4, Loss: 1.8904609917722628\n",
            "Training Size: 35000, Epoch: 5, Loss: 1.839590793226929\n",
            "Training Size: 35000, Perplexity: 5.9110236167907715\n",
            "Results: {1000: 17.348674774169922, 5000: 12.298166275024414, 10000: 10.060457229614258, 15000: 8.110535621643066, 20000: 7.5504984855651855, 25000: 7.670215129852295, 30000: 6.471180438995361, 35000: 5.9110236167907715}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {1000: 17.348674774169922, 5000: 12.298166275024414, 10000: 10.060457229614258, 15000: 8.110535621643066, 20000: 7.5504984855651855, 25000: 7.670215129852295, 30000: 6.471180438995361, 35000: 5.9110236167907715}"
      ],
      "metadata": {
        "id": "R2RQVtP5xprs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}