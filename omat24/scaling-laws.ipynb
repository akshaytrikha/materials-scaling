{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec50ccd-9f49-4030-b6bc-0dfc0531bc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# keep refreshing the env to update experiments.json\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# make cells take up the whole width to display graphs better\n",
    "from IPython.display import display, HTML, Markdown\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from scipy.optimize import curve_fit\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67393f08-f57a-4813-901b-9598f4f50543",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_filenames  = [\n",
    "    \"experiments_20250131_160623.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ff15b-f94b-4616-8236-a670a04520e3",
   "metadata": {},
   "source": [
    "### Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a26ade-a194-4649-98b4-56845a66a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(filename):\n",
    "    \"\"\"\n",
    "    Load and process the experimental data from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        summary_val (dict): Best achievable validation loss per dataset size.\n",
    "        detailed_runs (dict): Detailed runs data structured by dataset size.\n",
    "        loaded_data (dict): The raw loaded JSON data.\n",
    "    \"\"\"\n",
    "    with open(f\"results/{filename}\", \"r\") as f:\n",
    "        loaded_data = json.load(f)\n",
    "\n",
    "    detailed_runs = {}  # {dataset_size: [(params, best_val_loss, best_train_loss), ...]}\n",
    "    for ds_size_str, runs in loaded_data.items():\n",
    "        ds_size = float(ds_size_str)\n",
    "        run_losses = []\n",
    "        for run in runs:\n",
    "            val_losses = [step_data[\"val_loss\"] for step_data in run[\"losses\"].values()]\n",
    "            train_losses = [step_data[\"train_loss\"] for step_data in run[\"losses\"].values()]\n",
    "            best_val_loss = min(val_losses)\n",
    "            best_train_loss = min(train_losses)\n",
    "            num_params = run[\"config\"][\"num_params\"]\n",
    "            run_losses.append((num_params, best_val_loss, best_train_loss, run))  # Store run data for callbacks\n",
    "        detailed_runs[ds_size] = run_losses\n",
    "\n",
    "    # Create summary_results for scaling law plot\n",
    "    summary_val = {}  # best achievable validation loss per dataset size\n",
    "    for ds_size, runs in detailed_runs.items():\n",
    "        best_val = min(l[1] for l in runs)\n",
    "        summary_val[ds_size] = best_val\n",
    "\n",
    "    return summary_val, detailed_runs, loaded_data\n",
    "\n",
    "def prepare_scaling_data(summary_val):\n",
    "    \"\"\"\n",
    "    Prepare and sort the scaling data.\n",
    "\n",
    "    Args:\n",
    "        summary_val (dict): Best achievable validation loss per dataset size.\n",
    "\n",
    "    Returns:\n",
    "        sorted_ds_sizes (np.array): Sorted dataset sizes.\n",
    "        sorted_val_losses (np.array): Validation losses sorted accordingly.\n",
    "    \"\"\"\n",
    "    ds_sizes = np.array(list(summary_val.keys()))\n",
    "    val_losses = np.array(list(summary_val.values()))\n",
    "    sorted_indices = np.argsort(ds_sizes)\n",
    "    sorted_ds_sizes = ds_sizes[sorted_indices]\n",
    "    sorted_val_losses = val_losses[sorted_indices]\n",
    "    return sorted_ds_sizes, sorted_val_losses\n",
    "\n",
    "def calculate_scaling_law(ds_sizes, val_losses):\n",
    "    \"\"\"\n",
    "    Calculate the scaling law parameters and fitted values.\n",
    "\n",
    "    Args:\n",
    "        ds_sizes (np.array): Array of dataset sizes.\n",
    "        val_losses (np.array): Array of validation losses.\n",
    "\n",
    "    Returns:\n",
    "        fitted_vals (np.array): Fitted validation loss values based on scaling law.\n",
    "        a (float): Scaling law parameter a.\n",
    "        b (float): Scaling law parameter b.\n",
    "    \"\"\"\n",
    "    # Define the power-law function\n",
    "    def power_law(x, a, b):\n",
    "        return a * x ** (-b)\n",
    "\n",
    "    # Fit the power-law curve to the data\n",
    "    popt, _ = curve_fit(power_law, ds_sizes, val_losses, p0=(1, 1))  # Initial guess for a and b\n",
    "\n",
    "    # Extract parameters\n",
    "    a, b = popt\n",
    "\n",
    "    fitted_vals = power_law(ds_sizes, a, b)\n",
    "    return fitted_vals, a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93d03d-80f0-4719-886f-3345af43c1c2",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70be1fe3-8070-476e-a986-be458284072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ buggy ------------------ \n",
    "# all FCN models 50 epochs\n",
    "# experiment_filename = \"experiments_20250111_214659.json\"\n",
    "\n",
    "# all Transformer models 50 epochs                          ********\n",
    "# experiment_filename = \"experiments_20250112_062438.json\"\n",
    "\n",
    "# ------------------ bugfix ------------------ \n",
    "# all Transformer models 50 epochs  \n",
    "# experiment_filename = \"experiments_20250123_155336.json\"\n",
    "\n",
    "# ------------------ all loss equally scaled ------------------ \n",
    "# experiment_filename = \"experiments_20250123_162306.json\"\n",
    "\n",
    "\n",
    "# ------------------ low batch size (8) on 0.1 fraction ------------------ \n",
    "# experiment_filename = \"experiments_20250123_163741.json\"\n",
    "\n",
    "# ------------------ low batch size (8) ------------------ \n",
    "# 0.01 0.02 0.04 0.08\n",
    "# experiment_filename = \"experiments_20250124_102758.json\"\n",
    "# 0.1 0.2 0.4 0.8 1.0\n",
    "# experiment_filename = \"experiments_20250123_165735.json\"\n",
    "# experiment_filename_1 = \"merged.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ca9b9e-f5de-4220-ab12-975eef214fb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load and process each experiment file\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m experiment_filenames:\n\u001b[0;32m---> 12\u001b[0m     summary_val, detailed_runs, loaded_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     sorted_ds_sizes, sorted_val_losses \u001b[38;5;241m=\u001b[39m prepare_scaling_data(summary_val)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# fitted_vals, a, b = calculate_scaling_law(sorted_ds_sizes, sorted_val_losses)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Store data in dictionaries\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mload_and_process_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     19\u001b[0m run_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m runs:\n\u001b[0;32m---> 21\u001b[0m     val_losses \u001b[38;5;241m=\u001b[39m [step_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m step_data \u001b[38;5;129;01min\u001b[39;00m run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     22\u001b[0m     train_losses \u001b[38;5;241m=\u001b[39m [step_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m step_data \u001b[38;5;129;01min\u001b[39;00m run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     23\u001b[0m     best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(val_losses)\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m run_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m runs:\n\u001b[0;32m---> 21\u001b[0m     val_losses \u001b[38;5;241m=\u001b[39m [\u001b[43mstep_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step_data \u001b[38;5;129;01min\u001b[39;00m run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     22\u001b[0m     train_losses \u001b[38;5;241m=\u001b[39m [step_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m step_data \u001b[38;5;129;01min\u001b[39;00m run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     23\u001b[0m     best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(val_losses)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store data for all experiments\n",
    "all_summary_vals = {}\n",
    "all_detailed_runs = {}\n",
    "all_loaded_data = {}\n",
    "all_sorted_ds_sizes = {}\n",
    "all_sorted_val_losses = {}\n",
    "all_fitted_vals = {}\n",
    "all_scaling_params = {}\n",
    "\n",
    "# Load and process each experiment file\n",
    "for filename in experiment_filenames:\n",
    "    summary_val, detailed_runs, loaded_data = load_and_process_data(filename)\n",
    "    sorted_ds_sizes, sorted_val_losses = prepare_scaling_data(summary_val)\n",
    "    # fitted_vals, a, b = calculate_scaling_law(sorted_ds_sizes, sorted_val_losses)\n",
    "    \n",
    "    # Store data in dictionaries\n",
    "    all_summary_vals[filename] = summary_val\n",
    "    all_detailed_runs[filename] = detailed_runs\n",
    "    all_loaded_data[filename] = loaded_data\n",
    "    all_sorted_ds_sizes[filename] = sorted_ds_sizes\n",
    "    all_sorted_val_losses[filename] = sorted_val_losses\n",
    "    # all_fitted_vals[filename] = fitted_vals\n",
    "    # all_scaling_params[filename] = (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41617feb-3b70-439e-8cd3-b19c16caffff",
   "metadata": {},
   "source": [
    "### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5dfcaa-8584-425d-8f3f-bda16753e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Plot-1 (Scaling Law for Validation Loss)\n",
    "fig_plot1 = go.FigureWidget()\n",
    "\n",
    "# Define color and marker styles for different experiments\n",
    "color_palette = px.colors.qualitative.Plotly\n",
    "marker_symbols = ['circle', 'square', 'diamond', 'cross', 'triangle-up', 'triangle-down', 'star']\n",
    "\n",
    "color_map = {}\n",
    "marker_map = {}\n",
    "\n",
    "# Determine the overall min and max for dataset sizes to place baseline lines properly\n",
    "all_x_values = []\n",
    "for idx, filename in enumerate(experiment_filenames):\n",
    "    all_x_values.extend(all_sorted_ds_sizes[filename])\n",
    "min_x = min(all_x_values) if all_x_values else 0\n",
    "max_x = max(all_x_values) if all_x_values else 1\n",
    "\n",
    "# Ensure we don't get zero-range for lines\n",
    "if max_x == min_x:\n",
    "    max_x = min_x + 1\n",
    "\n",
    "for idx, filename in enumerate(experiment_filenames):\n",
    "    color = color_palette[idx % len(color_palette)]\n",
    "    marker = marker_symbols[idx % len(marker_symbols)]\n",
    "    color_map[filename] = color\n",
    "    marker_map[filename] = marker\n",
    "\n",
    "    # Extract data\n",
    "    sorted_ds_sizes = all_sorted_ds_sizes[filename]\n",
    "    sorted_val_losses = all_sorted_val_losses[filename]\n",
    "\n",
    "    # Original Data Trace\n",
    "    fig_plot1.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sorted_ds_sizes,\n",
    "            y=sorted_val_losses,\n",
    "            mode='lines+markers',\n",
    "            name=f'Best Val Loss<br>{filename}',\n",
    "            marker=dict(symbol=marker, size=8, color=color),\n",
    "            customdata=[{'experiment': filename, 'dataset_size': ds} for ds in sorted_ds_sizes],\n",
    "            hovertemplate='Dataset Size: %{x}<br>Val Loss: %{y}<extra></extra>',\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add the 3 baseline dashed lines\n",
    "fig_plot1.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[min_x, max_x],\n",
    "        y=[17.558, 17.558],\n",
    "        mode='lines',\n",
    "        line=dict(color='#808080', dash='dash'),\n",
    "        name='Naive 0 Everywhere at y= 17.558'\n",
    "    )\n",
    ")\n",
    "fig_plot1.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[min_x, max_x],\n",
    "        y=[6.484, 6.484],\n",
    "        mode='lines',\n",
    "        line=dict(color='#003262', dash='dash'),\n",
    "        name='Naive Mean at y= 6.484'\n",
    "    )\n",
    ")\n",
    "fig_plot1.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[min_x, max_x],\n",
    "        y=[5.696, 5.696],\n",
    "        mode='lines',\n",
    "        line=dict(color='#FDB515', dash='dash'),\n",
    "        name='Naive k=1 at y= 5.696'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_plot1.update_layout(\n",
    "    title=\"Scaling Law: Validation Loss vs Dataset Size\",\n",
    "    xaxis_title=\"Dataset Size\",\n",
    "    yaxis_title=\"Validation Loss\",\n",
    "    template=\"plotly_white\",\n",
    "    width=900,\n",
    "    height=600,\n",
    "    legend=dict(x=1.02, y=1)\n",
    ")\n",
    "\n",
    "# Add buttons for toggling axis scales\n",
    "xaxis_buttons = [\n",
    "    dict(args=[{\"xaxis.type\": \"linear\"}], label=\"X-Linear\", method=\"relayout\"),\n",
    "    dict(args=[{\"xaxis.type\": \"log\"}], label=\"X-Log\", method=\"relayout\")\n",
    "]\n",
    "yaxis_buttons = [\n",
    "    dict(args=[{\"yaxis.type\": \"linear\"}], label=\"Y-Linear\", method=\"relayout\"),\n",
    "    dict(args=[{\"yaxis.type\": \"log\"}], label=\"Y-Log\", method=\"relayout\")\n",
    "]\n",
    "fig_plot1.update_layout(\n",
    "    margin=dict(r=150),\n",
    "    updatemenus=[\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.02, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=xaxis_buttons, name=\"xaxis_buttons\"),\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.145, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=yaxis_buttons, name=\"yaxis_buttons\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Plot-2 (Saturation Curves for Train Loss)\n",
    "fig_train_loss = go.FigureWidget()\n",
    "fig_train_loss.update_layout(\n",
    "    title=\"Saturation Curves: Click a point in Plot-1 to view Train Loss curves\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Training Loss\",\n",
    "    template=\"plotly_white\",\n",
    "    width=720,\n",
    "    height=480,\n",
    ")\n",
    "fig_train_loss.update_layout(\n",
    "    margin=dict(r=150),\n",
    "    updatemenus=[\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.02, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=xaxis_buttons),\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.145, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=yaxis_buttons)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Plot-3 (Saturation Curves for Validation Loss)\n",
    "fig_val_loss = go.FigureWidget()\n",
    "fig_val_loss.update_layout(\n",
    "    title=\"Saturation Curves: Click a point in Plot-1 to view Val Loss curves\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Validation Loss\",\n",
    "    template=\"plotly_white\",\n",
    "    width=720,\n",
    "    height=480,\n",
    ")\n",
    "fig_val_loss.update_layout(\n",
    "    margin=dict(r=150),\n",
    "    updatemenus=[\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.02, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=yaxis_buttons),\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.145, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=yaxis_buttons)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Callback function to update saturation curves based on clicked point in Plot-1\n",
    "def update_saturation_curves(trace, points, selector):\n",
    "    if points.point_inds:\n",
    "        idx = points.point_inds[0]\n",
    "        clicked_data = trace.customdata[idx]\n",
    "        filename = clicked_data['experiment']\n",
    "        ds_size = clicked_data['dataset_size']\n",
    "\n",
    "        # Fetch the matching runs\n",
    "        detailed_runs = all_detailed_runs[filename]\n",
    "        runs = detailed_runs.get(ds_size, [])\n",
    "\n",
    "        # Clear old traces\n",
    "        fig_val_loss.data = []\n",
    "        fig_train_loss.data = []\n",
    "\n",
    "        for run_info in runs:\n",
    "            num_params, best_val_loss, best_train_loss, run_data = run_info\n",
    "            losses_dict = run_data[\"losses\"]\n",
    "\n",
    "            steps = []\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            for step_str, step_vals in losses_dict.items():\n",
    "                step_int = int(step_str)\n",
    "                steps.append(step_int)\n",
    "                train_losses.append(step_vals[\"train_loss\"])\n",
    "                val_losses.append(step_vals[\"val_loss\"])\n",
    "\n",
    "            # Sort by step for a proper left-to-right curve\n",
    "            sorted_idx = np.argsort(steps)\n",
    "            steps = [steps[i] for i in sorted_idx]\n",
    "            train_losses = [train_losses[i] for i in sorted_idx]\n",
    "            val_losses = [val_losses[i] for i in sorted_idx]\n",
    "\n",
    "            fig_train_loss.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=steps,\n",
    "                    y=train_losses,\n",
    "                    mode='lines+markers',\n",
    "                    name=f\"{num_params} params\"\n",
    "                )\n",
    "            )\n",
    "            fig_val_loss.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=steps,\n",
    "                    y=val_losses,\n",
    "                    mode='lines+markers',\n",
    "                    name=f\"{num_params} params\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Update titles\n",
    "        fig_train_loss.update_layout(\n",
    "            title=f\"Train Loss Curves for Dataset Size = {int(ds_size)}<br>({filename})\"\n",
    "        )\n",
    "        fig_val_loss.update_layout(\n",
    "            title=f\"Val Loss Curves for Dataset Size = {int(ds_size)}<br>({filename})\"\n",
    "        )\n",
    "\n",
    "# Attach the callback to the \"Best Val Loss\" traces\n",
    "for trace in fig_plot1.data:\n",
    "    if 'Best Val Loss' in trace.name:\n",
    "        trace.on_click(update_saturation_curves)\n",
    "\n",
    "# Arrange the plots in the desired layout\n",
    "top_row = widgets.VBox([fig_plot1])\n",
    "bottom_row = widgets.HBox([fig_train_loss, fig_val_loss])\n",
    "container = widgets.VBox([top_row, bottom_row])\n",
    "\n",
    "display(container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd8a83-c280-4220-888e-caae502a543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Plot-1 (Scaling Law for Validation Loss)\n",
    "fig_plot1 = go.FigureWidget()\n",
    "\n",
    "# Define color and marker styles for different experiments\n",
    "color_palette = px.colors.qualitative.Plotly\n",
    "marker_symbols = ['circle', 'square', 'diamond', 'cross', 'triangle-up', 'triangle-down', 'star']\n",
    "\n",
    "# To keep track of which colors and symbols are used\n",
    "color_map = {}\n",
    "marker_map = {}\n",
    "\n",
    "for idx, filename in enumerate(experiment_filenames):\n",
    "    color = color_palette[idx % len(color_palette)]\n",
    "    marker = marker_symbols[idx % len(marker_symbols)]\n",
    "    color_map[filename] = color\n",
    "    marker_map[filename] = marker\n",
    "    \n",
    "    # Extract data\n",
    "    sorted_ds_sizes = all_sorted_ds_sizes[filename]\n",
    "    sorted_val_losses = all_sorted_val_losses[filename]\n",
    "    # fitted_vals = all_fitted_vals[filename]\n",
    "    # a, b = all_scaling_params[filename]\n",
    "    \n",
    "    # Original Data Trace\n",
    "    fig_plot1.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sorted_ds_sizes, \n",
    "            y=sorted_val_losses, \n",
    "            mode='lines+markers', \n",
    "            name=f'Best Val Loss<br>{filename}',\n",
    "            marker=dict(symbol=marker, size=8, color=color),\n",
    "            customdata=[{'experiment': filename, 'dataset_size': ds} for ds in sorted_ds_sizes],\n",
    "            hovertemplate='Dataset Size: %{x}<br>Val Loss: %{y}<extra></extra>',\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # # Scaling Law Trace\n",
    "    # fig_plot1.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=sorted_ds_sizes, \n",
    "    #         y=fitted_vals, \n",
    "    #         mode='lines', \n",
    "    #         name=f'y = {a:.2f}x^(-{b:.2f})',\n",
    "    #         line=dict(dash='dash', color=color),\n",
    "    #         hovertemplate='Scaling Law: y = {:.2f}x^(-{:.2f})<extra></extra>'.format(a, b),\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "fig_plot1.update_layout(\n",
    "    title=\"Scaling Law: Validation Loss vs Dataset Size\",\n",
    "    xaxis_title=\"Dataset Size\",\n",
    "    yaxis_title=\"Validation Loss\",\n",
    "    template=\"plotly_white\",\n",
    "    width=900,\n",
    "    height=600,\n",
    "    legend=dict(x=1.02, y=1)\n",
    ")\n",
    "\n",
    "# Add buttons for toggling axis scales\n",
    "xaxis_buttons = [\n",
    "    dict(args=[{\"xaxis.type\": \"linear\"}], label=\"X-Linear\", method=\"relayout\"),\n",
    "    dict(args=[{\"xaxis.type\": \"log\"}], label=\"X-Log\", method=\"relayout\")\n",
    "]\n",
    "yaxis_buttons = [\n",
    "    dict(args=[{\"yaxis.type\": \"linear\"}], label=\"Y-Linear\", method=\"relayout\"),\n",
    "    dict(args=[{\"yaxis.type\": \"log\"}], label=\"Y-Log\", method=\"relayout\")\n",
    "]\n",
    "fig_plot1.update_layout(\n",
    "    margin=dict(r=150),\n",
    "    updatemenus=[\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.02, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=xaxis_buttons, name=\"xaxis_buttons\"),\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.145, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=yaxis_buttons, name=\"yaxis_buttons\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Plot-2 (Saturation Curves for Train Loss)\n",
    "fig_train_loss = go.FigureWidget()\n",
    "fig_train_loss.update_layout(\n",
    "    title=\"Saturation Curves: Click a point in Plot-1 to view Train Loss curves\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Training Loss\",\n",
    "    template=\"plotly_white\",\n",
    "    width=720,\n",
    "    height=480,\n",
    ")\n",
    "fig_train_loss.update_layout(\n",
    "    margin=dict(r=150),\n",
    "    updatemenus=[\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.02, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=xaxis_buttons),\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.145, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=yaxis_buttons)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Plot-3 (Saturation Curves for Validation Loss)\n",
    "fig_val_loss = go.FigureWidget()\n",
    "fig_val_loss.update_layout(\n",
    "    title=\"Saturation Curves: Click a point in Plot-1 to view Val Loss curves\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"Validation Loss\",\n",
    "    template=\"plotly_white\",\n",
    "    width=720,\n",
    "    height=480,\n",
    ")\n",
    "fig_val_loss.update_layout(\n",
    "    margin=dict(r=150),\n",
    "    updatemenus=[\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.02, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=yaxis_buttons),\n",
    "        dict(type=\"buttons\", direction=\"up\", x=1.145, y=0.05, xanchor=\"left\", yanchor=\"bottom\",\n",
    "             showactive=True, buttons=yaxis_buttons)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Callback function to update saturation curves based on clicked point in Plot-1\n",
    "def update_saturation_curves(trace, points, selector):\n",
    "    if points.point_inds:\n",
    "        idx = points.point_inds[0]\n",
    "        # Identify which dataset size & experiment was clicked\n",
    "        clicked_data = trace.customdata[idx]\n",
    "        filename = clicked_data['experiment']\n",
    "        ds_size = clicked_data['dataset_size']\n",
    "\n",
    "        # Fetch the matching runs\n",
    "        detailed_runs = all_detailed_runs[filename]\n",
    "        runs = detailed_runs.get(ds_size, [])\n",
    "\n",
    "        # Clear old traces\n",
    "        fig_val_loss.data = []\n",
    "        fig_train_loss.data = []\n",
    "\n",
    "        # For each run, just use the raw logging steps\n",
    "        for run_info in runs:\n",
    "            num_params, best_val_loss, best_train_loss, run_data = run_info\n",
    "            losses_dict = run_data[\"losses\"]\n",
    "\n",
    "            # Collect step numbers & losses (no epoch grouping)\n",
    "            steps = []\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            for step_str, step_vals in losses_dict.items():\n",
    "                step_int = int(step_str)\n",
    "                steps.append(step_int)\n",
    "                train_losses.append(step_vals[\"train_loss\"])\n",
    "                val_losses.append(step_vals[\"val_loss\"])\n",
    "\n",
    "            # Sort by step for a proper left-to-right curve\n",
    "            sorted_idx = np.argsort(steps)\n",
    "            steps = [steps[i] for i in sorted_idx]\n",
    "            train_losses = [train_losses[i] for i in sorted_idx]\n",
    "            val_losses = [val_losses[i] for i in sorted_idx]\n",
    "\n",
    "            # Plot train loss vs raw step\n",
    "            fig_train_loss.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=steps,\n",
    "                    y=train_losses,\n",
    "                    mode='lines+markers',\n",
    "                    name=f\"{num_params} params\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Plot val loss vs raw step\n",
    "            fig_val_loss.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=steps,\n",
    "                    y=val_losses,\n",
    "                    mode='lines+markers',\n",
    "                    name=f\"{num_params} params\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Update titles\n",
    "        fig_train_loss.update_layout(\n",
    "            title=f\"Train Loss Curves for Dataset Size = {int(ds_size)}<br>({filename})\"\n",
    "        )\n",
    "        fig_val_loss.update_layout(\n",
    "            title=f\"Val Loss Curves for Dataset Size = {int(ds_size)}<br>({filename})\"\n",
    "        )\n",
    "\n",
    "# Attach the callback to the data traces (only the \"Best Val Loss\" traces)\n",
    "for trace in fig_plot1.data:\n",
    "    if 'Best Val Loss' in trace.name:\n",
    "        trace.on_click(update_saturation_curves)\n",
    "\n",
    "# Arrange the plots in the desired layout\n",
    "top_row = widgets.VBox([fig_plot1])  # Scaling Law on top\n",
    "bottom_row = widgets.HBox([fig_train_loss, fig_val_loss])  # Training loss (left), Validation loss (right)\n",
    "container = widgets.VBox([top_row, bottom_row])\n",
    "\n",
    "# Display the updated layout\n",
    "display(container)\n",
    "\n",
    "# Optionally, display the scaling law parameters for all experiments\n",
    "# for filename in experiment_filenames:\n",
    "#     a, b = all_scaling_params[filename]\n",
    "#     print(f\"Scaling Law Parameters for {filename}:\\na = {a:.4f}\\nb = {b:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7463c01-8455-4e75-bb47-c35e8a2ab099",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_law_md = f\"\"\"\n",
    "### Scaling Law\n",
    "\n",
    "\n",
    "$y = {a:.4f} \\\\cdot x^{{-{b:.4f}}}$\n",
    "\n",
    "Where:\n",
    "- $a$: The scaling constant, representing the loss when $x = 1$.\n",
    "- $b$: The scaling exponent, describing how the loss decreases with dataset size.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(scaling_law_md))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
