{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairchem.core.datasets import AseDBDataset\n",
    "\n",
    "dataset_path = \"../datasets/val/rattled-500-subsampled\"\n",
    "config_kwargs = {}  # see tutorial on additional configuration\n",
    "\n",
    "ase_dataset = AseDBDataset(config=dict(src=dataset_path, **config_kwargs))\n",
    "\n",
    "# atoms objects can be retrieved by index\n",
    "atoms = ase_dataset.get_atoms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_atoms = [ase_dataset.get_atoms(i) for i in range(len(ase_dataset.ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [atoms.get_forces() for atoms in all_atoms]\n",
    "X_coords = [\n",
    "    np.concatenate(\n",
    "        [atoms.get_positions(wrap=True), atoms.get_scaled_positions(wrap=True)], axis=1\n",
    "    )\n",
    "    for atoms in all_atoms\n",
    "]\n",
    "X_numbers = [atoms.get_atomic_numbers() for atoms in all_atoms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels[0]), len(X_coords[0]), len(X_numbers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_coords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OMat24Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_coords, X_numbers, y):\n",
    "        self.X_coords = X_coords\n",
    "        self.X_numbers = X_numbers\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_numbers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.X_coords[idx],\n",
    "            self.X_numbers[idx],\n",
    "            self.y[idx],\n",
    "            torch.tensor([1] * len(self.y[idx]), dtype=torch.int64),\n",
    "        )\n",
    "\n",
    "\n",
    "class OMat24DataLoader(torch.utils.data.DataLoader):\n",
    "    # Sequences are different\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X_coords, X_numbers, labels, mask = zip(*batch)\n",
    "    # print(len(X_numbers[0]), len(X_coords[0]), len(labels[0]))\n",
    "    # print(len(X_numbers[1]), len(X_coords[1]), len(labels[1]))\n",
    "\n",
    "    X_coords_t = pad_sequence(\n",
    "        [torch.tensor(c, dtype=torch.float32) for c in X_coords], batch_first=True\n",
    "    )\n",
    "    X_numbers_t = pad_sequence(\n",
    "        [torch.tensor(n, dtype=torch.int64) for n in X_numbers], batch_first=True\n",
    "    )\n",
    "    labels_t = pad_sequence(\n",
    "        [torch.tensor(y, dtype=torch.float32) for y in labels], batch_first=True\n",
    "    )\n",
    "    mask_t = pad_sequence(\n",
    "        [torch.tensor(m, dtype=torch.int64) for m in mask], batch_first=True\n",
    "    ).to(torch.bool)\n",
    "\n",
    "    return X_coords_t, X_numbers_t, labels_t, mask_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings,\n",
    "        embedding_dim,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        dim_feedforward,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                embedding_dim + 6, num_heads, dim_feedforward, dropout, batch_first=True\n",
    "            ),\n",
    "            num_layers,\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim + 6, 3)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        # x = self.embedding(numbers+1)\n",
    "        numbers, coords = inputs\n",
    "        x = torch.cat([self.embedding(numbers), coords], dim=-1)\n",
    "        x = self.transformer(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformer(200, 32, 2, 3, 128, 0.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "losses = []\n",
    "large_losses = []\n",
    "small_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of model parameters\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dataset = OMATDataset(X_coords, X_numbers, labels)\n",
    "dataloader = OMATDataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    to_iter = tqdm(dataloader)\n",
    "    for i, (coords, numbers, forces, mask) in enumerate(to_iter):\n",
    "        to_iter.set_description(f\"Loss: {np.mean(losses[-100:]):.4f}\")\n",
    "        optimizer.zero_grad()\n",
    "        flat_mask = mask.flatten()\n",
    "        label_forces = forces.reshape(-1, 3)[flat_mask]\n",
    "        output_forces = model((numbers, coords), mask).reshape(-1, 3)[flat_mask]\n",
    "        # loss = F.mse_loss(label_forces.flatten(), output_forces.flatten(), reduction=\"mean\")\n",
    "        # diff_vec = label_forces.flatten() - output_forces.flatten()\n",
    "        # large_diffs = diff_vec[torch.abs(diff_vec) > 5]\n",
    "        # rest_diffs = diff_vec[torch.abs(diff_vec) <= 5]\n",
    "        # large_loss = torch.norm(large_diffs, p=0.75) / (len(large_diffs) ** (1/0.75))\n",
    "        # rest_loss = torch.norm(rest_diffs, p=2) / len(rest_diffs)**(1/2)\n",
    "        # loss = large_loss + rest_loss\n",
    "        # loss = torch.norm(diff_vec, p=0.75) / len(diff_vec)**(1/0.75)\n",
    "        loss = torch.mean(torch.abs(label_forces - output_forces))\n",
    "        loss.backward()\n",
    "        # clip gradients\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        # large_losses.append(large_loss.item())\n",
    "        # small_losses.append(rest_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_errors = []\n",
    "to_iter = tqdm(dataloader)\n",
    "for i, (coords, numbers, forces, mask) in enumerate(to_iter):\n",
    "    with torch.no_grad():\n",
    "        to_iter.set_description(f\"MAE: {np.mean(absolute_errors):.4f}\")\n",
    "        flat_mask = mask.flatten()\n",
    "        label_forces = forces.reshape(-1, 3)[flat_mask]\n",
    "        output_forces = model((numbers, coords), mask).reshape(-1, 3)[flat_mask]\n",
    "        diff_vec = label_forces.flatten() - output_forces.flatten()\n",
    "        absolute_errors.append(torch.mean(torch.abs(diff_vec)).item() * 3)\n",
    "        # # to_iter.set_description(f\"Loss: {np.mean(losses[-100:]):.4f}, Large Loss: {np.mean(large_losses[-100:]):.4f}, Small Loss: {np.mean(small_losses[-100:]):.4f}\")\n",
    "        # optimizer.zero_grad()\n",
    "        # flat_mask = mask.flatten()\n",
    "        # label_forces = forces.reshape(-1,3)[flat_mask]\n",
    "        # output_forces = model((numbers,coords), mask).reshape(-1,3)[flat_mask]\n",
    "        # # loss = F.mse_loss(label_forces.flatten(), output_forces.flatten(), reduction=\"mean\")\n",
    "        # diff_vec = label_forces.flatten() - output_forces.flatten()\n",
    "        # large_diffs = diff_vec[torch.abs(diff_vec) > 5]\n",
    "        # rest_diffs = diff_vec[torch.abs(diff_vec) <= 5]\n",
    "        # large_loss = torch.norm(large_diffs, p=0.5) / (len(large_diffs) ** 2.5)\n",
    "        # rest_loss = torch.mean(torch.abs(rest_diffs))\n",
    "        # loss = large_loss + rest_loss\n",
    "        # # loss = torch.norm(diff_vec, p=0.75) / len(diff_vec)**(1/0.75)\n",
    "        # loss.backward()\n",
    "        # # clip gradients\n",
    "        # # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        # optimizer.step()\n",
    "        # losses.append(loss.item())\n",
    "        # large_losses.append(large_loss.item())\n",
    "        # small_losses.append(rest_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_0_errors = []\n",
    "to_iter = tqdm(dataloader)\n",
    "for i, (coords, numbers, forces, mask) in enumerate(to_iter):\n",
    "    with torch.no_grad():\n",
    "        to_iter.set_description(f\"MAE: {np.mean(pred_0_errors):.4f}\")\n",
    "        flat_mask = mask.flatten()\n",
    "        label_forces = forces.reshape(-1, 3)[flat_mask]\n",
    "        output_forces = torch.zeros_like(label_forces)\n",
    "        diff_vec = label_forces.flatten() - output_forces.flatten()\n",
    "        pred_0_errors.append(torch.mean(torch.abs(diff_vec)).item() * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(absolute_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(output_forces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(label_forces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms.info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms.get_forces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OMat24Dataset(X_coords, X_numbers, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"dataset.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset) * 0.8)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_split, len(dataset) - train_split]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "\n",
    "with open(\"test_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset) * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "materials-scaling-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
